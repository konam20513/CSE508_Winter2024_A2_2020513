{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "csv_file_path = 'A2_Data.csv' \n",
    "destination_folder = 'downloaded_images'\n",
    "\n",
    "# Ensure destination folder exists\n",
    "Path(destination_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fetch_and_store_image(image_url, output_path):\n",
    "    try:\n",
    "        # Derive a filename by parsing the image URL\n",
    "        filename = os.path.basename(urlparse(image_url).path)\n",
    "        # Define the full path for saving the image\n",
    "        full_path = os.path.join(output_path, filename)\n",
    "        # Download and save the image using TensorFlow's utility\n",
    "        image_path = tf.keras.utils.get_file(fname=full_path, origin=image_url, cache_dir=destination_folder)\n",
    "        print(f\"Image saved at: {image_path}\")\n",
    "    except Exception as error:\n",
    "        print(f\"Failed to download {image_url} due to {error}\")\n",
    "\n",
    "\n",
    "def process_csv_and_download_images(csv_path, output_folder):\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(csv_path)\n",
    "\n",
    "    # Iterate through the dataset rows\n",
    "    for _, row in dataset.iterrows():\n",
    "        product_id = row['number']\n",
    "        image_urls = eval(row['Image'])  # Converts string representation of list to list\n",
    "\n",
    "        # Process each image URL\n",
    "        for url in image_urls:\n",
    "            # Construct a unique output path for each image\n",
    "            output_path = os.path.join(output_folder, str(product_id))\n",
    "            Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "            # Download and save the image\n",
    "            fetch_and_store_image(url, output_path)\n",
    "\n",
    "process_csv_and_download_images(csv_file_path, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomContrast\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Define the model for feature extraction\n",
    "def get_feature_extraction_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "        RandomFlip(\"horizontal\"),  # Random horizontal flip\n",
    "        RandomRotation(0.1),       # Random rotation\n",
    "        RandomContrast(0.1),       # Random contrast\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D()  # Add pooling layer to flatten the output\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to preprocess and augment an image\n",
    "def preprocess_and_augment_image(img_path):\n",
    "    img = load_img(img_path, target_size=(224, 224))  # Resize images\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)  # Apply ResNet50 preprocessing which includes normalization\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Process all images in a folder and save their extracted features\n",
    "def process_images_and_save_features(input_dir, output_dir, model):\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for img_file in os.listdir(input_dir):\n",
    "        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(input_dir, img_file)\n",
    "            img_array = preprocess_and_augment_image(img_path)\n",
    "            features = model.predict(img_array)\n",
    "            \n",
    "            # Save the features\n",
    "            feature_file = os.path.splitext(img_file)[0] + '.pkl'\n",
    "            with open(os.path.join(output_dir, feature_file), 'wb') as f:\n",
    "                pickle.dump(features, f)\n",
    "                \n",
    "            print(f\"Features extracted and saved for {img_file}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify directories\n",
    "    input_dir = 'images' \n",
    "    output_dir = 'image_features' \n",
    "    \n",
    "    # Initialize the feature extraction model\n",
    "    model = get_feature_extraction_model()\n",
    "    \n",
    "    # Process images and save features\n",
    "    process_images_and_save_features(input_dir, output_dir, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All reviews processed and TF-IDF scores saved.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import pickle\n",
    "\n",
    "# Initialize spaCy for English language\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    # Check if text is not a string (e.g., NaN or float)\n",
    "    if not isinstance(text, str):\n",
    "        return []  # Return an empty list of tokens for non-string inputs\n",
    "    \n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Function to calculate TF (Term Frequency)\n",
    "def calculate_tf(tokens):\n",
    "    tf = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        tf[token] += 1\n",
    "    return tf\n",
    "\n",
    "# Function to calculate IDF (Inverse Document Frequency)\n",
    "def calculate_idf(docs):\n",
    "    idf = defaultdict(lambda: 0)\n",
    "    total_docs = len(docs)\n",
    "    for doc in docs:\n",
    "        for token in set(doc):\n",
    "            idf[token] += 1\n",
    "    for token, val in idf.items():\n",
    "        idf[token] = log(total_docs / float(val))\n",
    "    return idf\n",
    "\n",
    "# Function to calculate TF-IDF\n",
    "def calculate_tfidf(docs):\n",
    "    tfidf_scores = []\n",
    "    idf = calculate_idf(docs)\n",
    "    for doc in docs:\n",
    "        tf = calculate_tf(doc)\n",
    "        tfidf = {token: tf_val * idf[token] for token, tf_val in tf.items()}\n",
    "        tfidf_scores.append(tfidf)\n",
    "    return tfidf_scores\n",
    "\n",
    "# Function to process all reviews and save their TF-IDF scores\n",
    "def process_reviews_and_save_tfidf(csv_file_path, output_dir):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Preprocess text reviews\n",
    "    preprocessed_reviews = [preprocess_text(text) for text in df['Review Text']]\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    tfidf_scores = calculate_tfidf(preprocessed_reviews)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Save TF-IDF scores with corresponding product IDs as filenames\n",
    "    for index, scores in enumerate(tfidf_scores):\n",
    "        product_id = df.iloc[index]['number']\n",
    "        output_file_path = os.path.join(output_dir, f\"{product_id}_text_tfidf.pkl\")\n",
    "        with open(output_file_path, 'wb') as f:\n",
    "            pickle.dump(scores, f)\n",
    "            \n",
    "    print(\"All reviews processed and TF-IDF scores saved.\")\n",
    "\n",
    "csv_file_path = 'A2_Data.csv'  \n",
    "output_dir = 'text_features' \n",
    "\n",
    "# Process reviews and save their TF-IDF scores\n",
    "process_reviews_and_save_tfidf(csv_file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path to the folder containing the image features\n",
    "image_folder_path = 'image_features'\n",
    "text_folder_path = 'text_features'\n",
    "text_folder2_path = 'text_features2'\n",
    "\n",
    "# Initialize an empty set to store unique IDs\n",
    "unique_ids = set()\n",
    "\n",
    "# Iterate over the files in the image features folder\n",
    "for filename in os.listdir(image_folder_path):\n",
    "    if os.path.isfile(os.path.join(image_folder_path, filename)):\n",
    "        # Split the filename by '_' and get the first part as the ID\n",
    "        file_id = filename.split('_')[0]\n",
    "        unique_ids.add(file_id)\n",
    "\n",
    "# Create a new folder to store selected text features\n",
    "os.makedirs(text_folder2_path, exist_ok=True)\n",
    "\n",
    "# Iterate over the files in the text features folder\n",
    "for filename in os.listdir(text_folder_path):\n",
    "    if os.path.isfile(os.path.join(text_folder_path, filename)):\n",
    "        # Split the filename by '_' and get the first part as the ID\n",
    "        file_id = filename.split('_')[0]\n",
    "        # Check if the ID is in the final IDs set\n",
    "        if file_id in unique_ids:\n",
    "            # Copy the file to the new folder\n",
    "            shutil.copy(os.path.join(text_folder_path, filename), os.path.join(text_folder2_path, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "final_ids=unique_ids\n",
    "# Define the filename for the initial CSV file\n",
    "initial_csv_filename = 'A2_data.csv'\n",
    "\n",
    "# Define the filename for the final CSV file\n",
    "final_csv_filename = 'A2_data_filtered.csv'\n",
    "\n",
    "# Open the initial CSV file for reading and the final CSV file for writing\n",
    "with open(initial_csv_filename, 'r', newline='') as initial_file, \\\n",
    "        open(final_csv_filename, 'w', newline='') as final_file:\n",
    "    \n",
    "    # Create CSV reader and writer objects\n",
    "    csv_reader = csv.DictReader(initial_file)\n",
    "    csv_writer = csv.DictWriter(final_file, fieldnames=csv_reader.fieldnames)\n",
    "    \n",
    "    # Write the header row to the final CSV file\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    # Iterate over each row in the initial CSV file\n",
    "    for row in csv_reader:\n",
    "        # Check if the number in the row is present in the final_ids set\n",
    "        if row['number'] in final_ids:\n",
    "            # Write the row to the final CSV file\n",
    "            csv_writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to calculate cosine similarity for arrays\n",
    "def cosine_similarity_vec(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_a = np.linalg.norm(v1)\n",
    "    norm_b = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a and norm_b else 0\n",
    "\n",
    "# Function to calculate cosine similarity for dictionaries (text features)\n",
    "def cosine_similarity_dict(d1, d2):\n",
    "    words = set(d1.keys()).intersection(set(d2.keys()))\n",
    "    dot_product = sum(d1[word] * d2[word] for word in words)\n",
    "    norm_d1 = np.sqrt(sum(v * v for v in d1.values()))\n",
    "    norm_d2 = np.sqrt(sum(v * v for v in d2.values()))\n",
    "    return dot_product / (norm_d1 * norm_d2) if norm_d1 and norm_d2 else 0\n",
    "\n",
    "# Load features from a pickle file\n",
    "def load_features_from_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def process_and_save_metrics(csv_file_path, image_features_dir, text_features_dir, output_dir):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        product_id = row['number']\n",
    "        image_urls = eval(row['Image'])\n",
    "\n",
    "        # Load text features for the product ID\n",
    "        text_feature_path = Path(text_features_dir) / f\"{product_id}_text_tfidf.pkl\"\n",
    "        text_features = pickle.load(open(text_feature_path, 'rb'))\n",
    "\n",
    "        # Load all other text and image features\n",
    "        all_text_features = {file.stem.split('_')[0]: pickle.load(open(file, 'rb'))\n",
    "                             for file in Path(text_features_dir).glob('*_text_tfidf.pkl') if file.stem != f\"{product_id}_text_tfidf\"}\n",
    "        all_image_features = {file.stem.split('_')[0]: np.array(pickle.load(open(file, 'rb'))[0])\n",
    "                              for file in Path(image_features_dir).glob('*.pkl') if not file.stem.startswith(f\"{product_id}_\")}\n",
    "\n",
    "        # Process each image URL for the product ID\n",
    "        for img_idx, img_url in enumerate(image_urls):\n",
    "            image_feature_path = Path(image_features_dir) / f\"{product_id}_{img_idx}.pkl\"\n",
    "            image_features = np.array(pickle.load(open(image_feature_path, 'rb'))[0])  # Assuming the feature is the first element\n",
    "\n",
    "            # Find similar IDs based on text and image features\n",
    "            similar_by_text = [(other_id, cosine_similarity_dict(text_features, all_text_features[other_id]))\n",
    "                               for other_id in all_text_features]\n",
    "            similar_by_image = [(other_id, cosine_similarity_vec(image_features, all_image_features[other_id]))\n",
    "                                for other_id in all_image_features]\n",
    "\n",
    "            # Sort and get top 3 similar IDs\n",
    "            similar_by_text.sort(key=lambda x: x[1], reverse=True)\n",
    "            similar_by_image.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_similar_by_text = similar_by_text[:3]\n",
    "            top_similar_by_image = similar_by_image[:3]\n",
    "\n",
    "            # Save cosine similarity scores for both text and image\n",
    "            metrics = {\n",
    "                'text_based': [(other_id, \n",
    "                                cosine_similarity_vec(image_features, all_image_features[other_id]),\n",
    "                                text_similarity\n",
    "                                )\n",
    "                               for other_id, text_similarity in top_similar_by_text],\n",
    "                'image_based': [(other_id, \n",
    "                                 image_similarity, \n",
    "                                 cosine_similarity_dict(text_features, all_text_features[other_id]))\n",
    "                                for other_id, image_similarity in top_similar_by_image]\n",
    "            }\n",
    "\n",
    "            # Save to pickle file\n",
    "            output_filename = f\"metrics_{product_id}_{img_idx}.pkl\"\n",
    "            output_filepath = Path(output_dir) / output_filename\n",
    "            with open(output_filepath, 'wb') as f:\n",
    "                pickle.dump(metrics, f)\n",
    "            print(f\"Metrics saved for product ID {product_id} image index {img_idx} to {output_filename}\")\n",
    "\n",
    "\n",
    "# Run the main process\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = 'A2_Data_filtered.csv'  \n",
    "    image_features_dir = 'image_features' \n",
    "    text_features_dir = 'text_features2'  \n",
    "    output_dir = 'output_metrics'  \n",
    "\n",
    "    process_and_save_metrics(csv_file_path, image_features_dir, text_features_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—--------------------------------------------------------------------------------------------\n",
      "USING IMAGE RETRIEVAL\n",
      "1) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg']\n",
      "Review: These locking tuners look great and keep tune.  Good quality materials and construction.  Excellent upgrade to any guitar.  I had to drill additions holes for installation.  If your neck already comes with pre-drilled holes, then they should drop right in, otherwise you will need to buy a guitar tuner pin drill jig, also available from Amazon.\n",
      "Cosine similarity of images: 0.7491\n",
      "Cosine similarity of text: 0.1401\n",
      "Composite similarity score: 0.4446\n",
      "2) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/711kGbkdzEL._SY88.jpg']\n",
      "Review: Had to drill into my headstock. Needs 2 holes per tree because of the mounting peg. Use a ruler and a 1/16 drillbit and you'll be fine. I recommend installing with the strings on so you can set them properly.\n",
      "Cosine similarity of images: 0.7042\n",
      "Cosine similarity of text: 0.0000\n",
      "Composite similarity score: 0.3521\n",
      "3) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/61WsGzCckEL._SY88.jpg']\n",
      "Review: Can't say they look exactly like what's in the picture, especially since they don't have that gold ring around the center. However they're still perfectly usable, cheap, and still look neat on the guitar\n",
      "Cosine similarity of images: 0.6482\n",
      "Cosine similarity of text: 0.0000\n",
      "Composite similarity score: 0.3241\n",
      "—--------------------------------------------------------------------------------------------\n",
      "USING TEXT RETRIEVAL\n",
      "1) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/61DvLcapd8L._SY88.jpg']\n",
      "Review: I went from fender chrome non-locking to fender gold locking. It made my guitar look beautiful and play beautiful. I think locking tuners are the way to go. If you are new to locking tuners look on YouTube for instructions.\n",
      "Cosine similarity of images: 0.5432\n",
      "Cosine similarity of text: 0.2231\n",
      "Composite similarity score: 0.3831\n",
      "2) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/51Mqwv4MnAL._SY88.jpg']\n",
      "Review: I've had this tuner for about 4 years and it's been great. It tunes fine. For a long time the tuner just sat in my room, but recently I started taking the tuner around to practice and transporting it in my bag. Unfortunately one of the 3 plastic pieces where the gooseneck attaches to the tuner broke off. It still works but the head sometimes falls off of the tuner. Now I'm looking for a clip on tuner that is a little more durable. If anyone has any suggestions, please let me know.\n",
      "\n",
      "I've added a photo that shows where one of the plastic pieces is missing (so you can see the joint)\n",
      "Cosine similarity of images: 0.4433\n",
      "Cosine similarity of text: 0.2143\n",
      "Composite similarity score: 0.3288\n",
      "3) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/71mhnYAH5VL._SY88.jpg']\n",
      "Review: My Tele is perfect, thank you very much.\n",
      "Cosine similarity of images: 0.3923\n",
      "Cosine similarity of text: 0.2050\n",
      "Composite similarity score: 0.2987\n",
      "\n",
      "—--------------------------------------------------------------------------------------------\n",
      "TOP 3 COMBINED PAIRS BASED ON COMPOSITE SIMILARITY SCORE\n",
      "1) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg']\n",
      "Review: These locking tuners look great and keep tune.  Good quality materials and construction.  Excellent upgrade to any guitar.  I had to drill additions holes for installation.  If your neck already comes with pre-drilled holes, then they should drop right in, otherwise you will need to buy a guitar tuner pin drill jig, also available from Amazon.\n",
      "Cosine similarity of images: 0.7491\n",
      "Cosine similarity of text: 0.1401\n",
      "Composite similarity score: 0.4446\n",
      "2) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/61DvLcapd8L._SY88.jpg']\n",
      "Review: I went from fender chrome non-locking to fender gold locking. It made my guitar look beautiful and play beautiful. I think locking tuners are the way to go. If you are new to locking tuners look on YouTube for instructions.\n",
      "Cosine similarity of images: 0.5432\n",
      "Cosine similarity of text: 0.2231\n",
      "Composite similarity score: 0.3831\n",
      "3) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/711kGbkdzEL._SY88.jpg']\n",
      "Review: Had to drill into my headstock. Needs 2 holes per tree because of the mounting peg. Use a ruler and a 1/16 drillbit and you'll be fine. I recommend installing with the strings on so you can set them properly.\n",
      "Cosine similarity of images: 0.7042\n",
      "Cosine similarity of text: 0.0000\n",
      "Composite similarity score: 0.3521\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def match_input_with_dataset(csv_file_path, input_image_url, input_review_text):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    for _, row in df.iterrows():\n",
    "        if input_review_text == row['Review Text'] and input_image_url in eval(row['Image']):\n",
    "            return row['number'], eval(row['Image']).index(input_image_url)\n",
    "    raise ValueError(\"No matching entry found in the dataset.\")\n",
    "\n",
    "def load_specific_metrics(output_metrics_dir, product_id, image_index):\n",
    "    metrics_path = Path(output_metrics_dir) / f\"metrics_{product_id}_{image_index}.pkl\"\n",
    "    with open(metrics_path, 'rb') as f:\n",
    "        metrics = pickle.load(f)\n",
    "    return metrics\n",
    "\n",
    "def get_urls_and_reviews(csv_file_path, id):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    row = df.loc[df['number'] == int(id)].iloc[0]\n",
    "    return eval(row['Image']), row['Review Text']\n",
    "\n",
    "def calculate_composite_score(image_cosine, text_cosine):\n",
    "    return (image_cosine + text_cosine) / 2\n",
    "\n",
    "def display_formatted_results(metrics, type, csv_file_path):\n",
    "    print(f\"—--------------------------------------------------------------------------------------------\\nUSING {type.upper()} RETRIEVAL\")\n",
    "    for idx, (id, image_cosine, text_cosine) in enumerate(metrics, start=1):\n",
    "        composite_score = calculate_composite_score(image_cosine, text_cosine)\n",
    "        urls, review = get_urls_and_reviews(csv_file_path, id)\n",
    "        print(f\"{idx}) Image URL: {urls}\\nReview: {review}\\nCosine similarity of images: {image_cosine:.4f}\\nCosine similarity of text: {text_cosine:.4f}\\nComposite similarity score: {composite_score:.4f}\")\n",
    "\n",
    "def get_combined_top_pairs(image_based_metrics, text_based_metrics):\n",
    "    # Combine metrics from both retrieval types and calculate composite score for each\n",
    "    combined_metrics = [(id, image_cosine, text_cosine, calculate_composite_score(image_cosine, text_cosine)) for id, image_cosine, text_cosine in image_based_metrics + text_based_metrics]\n",
    "    # Sort by composite score\n",
    "    sorted_by_composite_score = sorted(combined_metrics, key=lambda x: x[3], reverse=True)\n",
    "    # Return top 3 pairs with all details\n",
    "    return sorted_by_composite_score[:3]\n",
    "\n",
    "def main(input_image_url, input_review_text, csv_file_path, output_metrics_dir):\n",
    "    product_id, image_index = match_input_with_dataset(csv_file_path, input_image_url, input_review_text)\n",
    "    metrics = load_specific_metrics(output_metrics_dir, product_id, image_index)\n",
    "\n",
    "    # Adjust to compute and display results correctly with composite scores\n",
    "    image_based_with_composite = [(id, img_cos, txt_cos) for id, img_cos, txt_cos in metrics['image_based']]\n",
    "    text_based_with_composite = [(id, img_cos, txt_cos) for id, img_cos, txt_cos in metrics['text_based']]\n",
    "\n",
    "    display_formatted_results(image_based_with_composite, 'image', csv_file_path)\n",
    "    display_formatted_results(text_based_with_composite, 'text', csv_file_path)\n",
    "\n",
    "    # Calculate top 3 combined pairs across both systems\n",
    "    top_combined_pairs = get_combined_top_pairs(image_based_with_composite, text_based_with_composite)\n",
    "\n",
    "    # Display top 3 combined pairs\n",
    "    print(\"\\n—--------------------------------------------------------------------------------------------\")\n",
    "    print(\"TOP 3 COMBINED PAIRS BASED ON COMPOSITE SIMILARITY SCORE\")\n",
    "    for idx, (id, image_cosine, text_cosine, composite_score) in enumerate(top_combined_pairs, start=1):\n",
    "        urls, review = get_urls_and_reviews(csv_file_path, id)\n",
    "        print(f\"{idx}) Image URL: {urls}\\nReview: {review}\\nCosine similarity of images: {image_cosine:.4f}\\nCosine similarity of text: {text_cosine:.4f}\\nComposite similarity score: {composite_score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_url = \"https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\"\n",
    "    input_review_text = \"I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\"\n",
    "    csv_file_path = \"A2_Data_filtered.csv\"\n",
    "    output_metrics_dir = \"output_metrics\"\n",
    "    main(input_image_url, input_review_text, csv_file_path, output_metrics_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
